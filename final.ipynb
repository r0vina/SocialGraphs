{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "base_url = \"https://ogjre.com/transcripts\"\n",
    "output_file = \"filtered_transcripts.txt\"\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Uncomment to run in headless mode\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def fetch_all_episodes(driver):\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)\n",
    "    print(\"Webpage loaded. Starting to scroll...\")\n",
    "\n",
    "    episodes = []\n",
    "    scroll_pause_time = 2\n",
    "    max_no_new_content_attempts = 10\n",
    "    no_new_content_attempts = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while no_new_content_attempts < max_no_new_content_attempts:\n",
    "        try:\n",
    "            episode_elements = driver.find_elements(By.CLASS_NAME, \"VideoSingle__VideoSingleStyles-sc-dngnuh-0\")\n",
    "            new_episodes_found = False\n",
    "\n",
    "            for episode in episode_elements:\n",
    "                try:\n",
    "                    link = episode.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                    title = episode.find_element(By.CLASS_NAME, \"vs-video-title\").text\n",
    "                    if (title, link) not in episodes:\n",
    "                        episodes.append((title, link))\n",
    "                        new_episodes_found = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting episode: {e}\")\n",
    "\n",
    "            if new_episodes_found:\n",
    "                no_new_content_attempts = 0\n",
    "            else:\n",
    "                no_new_content_attempts += 1\n",
    "\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scrolling: {e}\")\n",
    "            break\n",
    "\n",
    "    return episodes\n",
    "\n",
    "def fetch_transcript(episode_url):\n",
    "    print(f\"Fetching transcript for {episode_url}...\")\n",
    "    response = requests.get(episode_url)\n",
    "    if response.status_code != 200:\n",
    "        return \"Transcript not available.\"\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    transcript_tag = soup.find(\"p\", class_=\"chakra-text ssc-transcript css-0\")\n",
    "    return transcript_tag.get_text(strip=True) if transcript_tag else \"Transcript not available.\"\n",
    "\n",
    "def is_valid_episode_title(title):\n",
    "    return bool(re.match(r\"^#\\d+\\s*-\\s*.+\", title))\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        episodes = fetch_all_episodes(driver)\n",
    "        print(f\"Found {len(episodes)} episodes.\")\n",
    "\n",
    "        # Filter episodes by valid title format\n",
    "        valid_episodes = [(title, link) for title, link in episodes if is_valid_episode_title(title)]\n",
    "        print(f\"Found {len(valid_episodes)} valid episodes.\")\n",
    "\n",
    "        # Save filtered transcripts\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            for title, link in valid_episodes:\n",
    "                print(f\"Scraping episode: {title}\")\n",
    "                transcript = fetch_transcript(link)\n",
    "                file.write(f\"Episode Title: {title}\\n\")\n",
    "                file.write(f\"Transcript:\\n{transcript}\\n\")\n",
    "                file.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        print(f\"Filtered transcripts saved in {output_file}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to processed_transcripts.json\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove excessive whitespace\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Extract named entities (people)\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "# Perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment[\"compound\"]  # Return compound sentiment score\n",
    "\n",
    "# Preprocess each transcript\n",
    "def preprocess_transcripts(input_file, output_file):\n",
    "    processed_data = []\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        episodes = f.read().split(\"=\" * 80)  # Split episodes by separator\n",
    "\n",
    "    for episode in episodes:\n",
    "        if not episode.strip():\n",
    "            continue\n",
    "        lines = episode.strip().split(\"\\n\")\n",
    "        title = lines[0].replace(\"Episode Title: \", \"\").strip()\n",
    "        transcript = \"\\n\".join(lines[1:]).replace(\"Transcript:\\n\", \"\").strip()\n",
    "\n",
    "        cleaned_transcript = clean_text(transcript)\n",
    "        named_entities = extract_named_entities(transcript)\n",
    "        sentiment_score = analyze_sentiment(transcript)\n",
    "\n",
    "        processed_data.append({\n",
    "            \"Episode Title\": title,\n",
    "            \"Cleaned_Transcript\": cleaned_transcript,\n",
    "            \"Named_Entities\": named_entities,\n",
    "            \"Sentiment_Score\": sentiment_score\n",
    "        })\n",
    "\n",
    "    # Save processed data to a JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed_data, f, indent=4)\n",
    "\n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "preprocess_transcripts(\"joe_rogan_filtered_transcript.txt\", \"processed_transcripts.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
