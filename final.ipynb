{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/italorobello/Desktop/Social Graphs/Final assignment/jre_episodes.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 197\u001b[0m\n\u001b[1;32m    195\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjre_episodes_with_metrics.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m json_output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjre_episodes_data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mprocess_youtube_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_output_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 146\u001b[0m, in \u001b[0;36mprocess_youtube_data\u001b[0;34m(input_file, output_file, json_output_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_youtube_data\u001b[39m(input_file, output_file, json_output_file):\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Process YouTube video URLs from an Excel file, save scraped data, and generate JSON.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     driver \u001b[38;5;241m=\u001b[39m setup_driver()\n\u001b[1;32m    148\u001b[0m     processed_data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/excel/_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1557\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/excel/_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/italorobello/Desktop/Social Graphs/Final assignment/jre_episodes.xlsx'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Setup Chrome WebDriver with options.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    # Uncomment the following line to run in headless mode\n",
    "    # options.add_argument(\"--headless\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def wait_for_element(driver, by, value, timeout=15):\n",
    "    \"\"\"Wait for an element to be present on the page.\"\"\"\n",
    "    return WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, value)))\n",
    "\n",
    "\n",
    "def scrape_youtube_video_data(driver, url):\n",
    "    \"\"\"Scrape YouTube video data including likes, views, and people mentioned.\"\"\"\n",
    "    metrics = {\"Likes\": \"Not Found\", \"Views\": \"Not Found\", \"People_Mentioned\": []}\n",
    "    try:\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "        # Accept cookies if prompted\n",
    "        try:\n",
    "            cookie_button = wait_for_element(driver, By.XPATH, '//*[@id=\"content\"]/div[2]/div[6]/div[1]/ytd-button-renderer[1]/yt-button-shape/button/yt-touch-feedback-shape/div/div[2]', timeout=10)\n",
    "            cookie_button.click()\n",
    "            print(\"Cookies accepted.\")\n",
    "            time.sleep(2)\n",
    "        except Exception:\n",
    "            print(\"No cookies prompt or already accepted.\")\n",
    "\n",
    "        # Get likes\n",
    "        try:\n",
    "            likes_element = wait_for_element(driver, By.XPATH, '//*[@id=\"top-level-buttons-computed\"]/segmented-like-dislike-button-view-model/yt-smartimation/div/div/like-button-view-model/toggle-button-view-model/button-view-model/button/div[2]', 15)\n",
    "            metrics[\"Likes\"] = likes_element.text\n",
    "            print(f\"Likes: {metrics['Likes']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch likes: {e}\")\n",
    "\n",
    "        # Get views\n",
    "        try:\n",
    "            views_element = wait_for_element(driver, By.CSS_SELECTOR, '.style-scope.yt-formatted-string.bold[style-target=\"bold\"]', 15)\n",
    "            metrics[\"Views\"] = views_element.text\n",
    "            print(f\"Views: {metrics['Views']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch views: {e}\")\n",
    "\n",
    "        # Expand and retrieve names of people mentioned\n",
    "        try:\n",
    "            expand_button = wait_for_element(driver, By.XPATH, '//*[@id=\"expand\"]', timeout=10)\n",
    "            expand_button.click()\n",
    "            print(\"'Expand' button clicked. Scrolling down to reveal names.\")\n",
    "            time.sleep(2)  # Allow time for content to expand\n",
    "\n",
    "            # Scroll down to ensure all names are loaded\n",
    "            driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Retrieve names of people mentioned\n",
    "            index = 1\n",
    "            while True:\n",
    "                try:\n",
    "                    person_xpath = f'//*[@id=\"items\"]/yt-video-attributes-section-view-model/div/div[2]/div/yt-video-attribute-view-model[{index}]/div/a/div[2]/h1'\n",
    "                    person_element = wait_for_element(driver, By.XPATH, person_xpath, timeout=5)\n",
    "                    person_name = person_element.text.strip()\n",
    "                    metrics[\"People_Mentioned\"].append(person_name)\n",
    "                    print(f\"Person {index}: {person_name}\")\n",
    "                    index += 1\n",
    "                except Exception:\n",
    "                    print(f\"Finished retrieving people mentioned. Total: {len(metrics['People_Mentioned'])}\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving people mentioned: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def fetch_transcript(episode_url):\n",
    "    \"\"\"Fetch transcript from the given URL.\"\"\"\n",
    "    print(f\"Fetching transcript for {episode_url}...\")\n",
    "    try:\n",
    "        response = requests.get(episode_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        transcript_tag = soup.find(\"p\", class_=\"chakra-text ssc-transcript css-0\")\n",
    "        transcript = transcript_tag.get_text(strip=True) if transcript_tag else \"Transcript not available.\"\n",
    "\n",
    "        # Preprocess the transcript\n",
    "        return preprocess_transcript(transcript)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching transcript for {episode_url}: {e}\")\n",
    "        return \"Transcript not available.\"\n",
    "\n",
    "\n",
    "def preprocess_transcript(transcript):\n",
    "    \"\"\"Preprocess transcript by cleaning, lemmatizing, and tokenizing.\"\"\"\n",
    "    if transcript == \"Transcript not available.\":\n",
    "        return transcript\n",
    "\n",
    "    # Text Cleaning: Remove punctuation, stopwords, and lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    transcript = re.sub(r'[^a-zA-Z\\s]', '', transcript)  # Remove punctuation\n",
    "    words = word_tokenize(transcript.lower())\n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    cleaned_transcript = ' '.join(cleaned_words)\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(transcript)\n",
    "\n",
    "    return {\n",
    "        \"cleaned_transcript\": cleaned_transcript,\n",
    "        \"sentiment\": sentiment\n",
    "    }\n",
    "\n",
    "\n",
    "def process_youtube_data(input_file, output_file, json_output_file):\n",
    "    \"\"\"Process YouTube video URLs from an Excel file, save scraped data, and generate JSON.\"\"\"\n",
    "    df = pd.read_excel(input_file)\n",
    "    driver = setup_driver()\n",
    "    processed_data = []\n",
    "\n",
    "    try:\n",
    "        for index, row in df.iterrows():  # Process the full DataFrame\n",
    "            video_url = row['url']\n",
    "            generated_url = row['generated_url']\n",
    "            print(f\"Processing video: {row['name']}\")\n",
    "            try:\n",
    "                metrics = scrape_youtube_video_data(driver, video_url)\n",
    "                transcript_data = fetch_transcript(generated_url)\n",
    "\n",
    "                # Save the scraped data to the DataFrame\n",
    "                df.at[index, 'Likes'] = metrics[\"Likes\"]\n",
    "                df.at[index, 'Views'] = metrics[\"Views\"]\n",
    "                df.at[index, 'People_Mentioned'] = \", \".join(metrics[\"People_Mentioned\"])\n",
    "\n",
    "                # Append data for JSON file\n",
    "                processed_data.append({\n",
    "                    \"Episode Title\": row['name'],\n",
    "                    \"Cleaned_Transcript\": transcript_data[\"cleaned_transcript\"] if isinstance(transcript_data, dict) else transcript_data,\n",
    "                    \"people_mentioned\": metrics[\"People_Mentioned\"],\n",
    "                    \"views\": metrics[\"Views\"],\n",
    "                    \"likes\": metrics[\"Likes\"],\n",
    "                    \"guest\": row.get('guest', 'Unknown'),\n",
    "                    \"sentiment\": transcript_data.get(\"sentiment\") if isinstance(transcript_data, dict) else {}\n",
    "                })\n",
    "                print(f\"Successfully processed video: {row['name']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {row['name']}: {e}\")\n",
    "\n",
    "        # Save the updated DataFrame to an Excel file\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"Scraped data saved to {output_file}\")\n",
    "\n",
    "        # Save the processed data to a JSON file\n",
    "        with open(json_output_file, 'w') as json_file:\n",
    "            json.dump(processed_data, json_file, indent=4)\n",
    "        print(f\"Processed data saved to {json_output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"WebDriver closed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"jre_episodes.xlsx\"\n",
    "    output_file = \"jre_episodes_with_metrics.xlsx\"\n",
    "    json_output_file = \"jre_episodes_data.json\"\n",
    "    process_youtube_data(input_file, output_file, json_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Setup Chrome WebDriver with options.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    # Uncomment the following line to run in headless mode\n",
    "    # options.add_argument(\"--headless\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "def wait_for_element(driver, by, value, timeout=15):\n",
    "    \"\"\"Wait for an element to be present on the page.\"\"\"\n",
    "    return WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, value)))\n",
    "\n",
    "\n",
    "def scrape_youtube_video_data(driver, url):\n",
    "    \"\"\"Scrape YouTube video data including likes, views, and people mentioned.\"\"\"\n",
    "    metrics = {\"Likes\": \"Not Found\", \"Views\": \"Not Found\", \"People_Mentioned\": []}\n",
    "    try:\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "        # Accept cookies if prompted\n",
    "        try:\n",
    "            cookie_button = wait_for_element(driver, By.XPATH, '//*[@id=\"content\"]/div[2]/div[6]/div[1]/ytd-button-renderer[1]/yt-button-shape/button/yt-touch-feedback-shape/div/div[2]', timeout=10)\n",
    "            cookie_button.click()\n",
    "            print(\"Cookies accepted.\")\n",
    "            time.sleep(2)\n",
    "        except Exception:\n",
    "            print(\"No cookies prompt or already accepted.\")\n",
    "\n",
    "        # Get likes\n",
    "        try:\n",
    "            likes_element = wait_for_element(driver, By.XPATH, '//*[@id=\"top-level-buttons-computed\"]/segmented-like-dislike-button-view-model/yt-smartimation/div/div/like-button-view-model/toggle-button-view-model/button-view-model/button/div[2]', 15)\n",
    "            metrics[\"Likes\"] = likes_element.text\n",
    "            print(f\"Likes: {metrics['Likes']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch likes: {e}\")\n",
    "\n",
    "        # Get views\n",
    "        try:\n",
    "            views_element = wait_for_element(driver, By.CSS_SELECTOR, '.style-scope.yt-formatted-string.bold[style-target=\"bold\"]', 15)\n",
    "            metrics[\"Views\"] = views_element.text\n",
    "            print(f\"Views: {metrics['Views']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch views: {e}\")\n",
    "\n",
    "        # Expand and retrieve names of people mentioned\n",
    "        try:\n",
    "            expand_button = wait_for_element(driver, By.XPATH, '//*[@id=\"expand\"]', timeout=10)\n",
    "            expand_button.click()\n",
    "            print(\"'Expand' button clicked. Scrolling down to reveal names.\")\n",
    "            time.sleep(2)  # Allow time for content to expand\n",
    "\n",
    "            # Scroll down to ensure all names are loaded\n",
    "            driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Retrieve names of people mentioned\n",
    "            index = 1\n",
    "            while True:\n",
    "                try:\n",
    "                    person_xpath = f'//*[@id=\"items\"]/yt-video-attributes-section-view-model/div/div[2]/div/yt-video-attribute-view-model[{index}]/div/a/div[2]/h1'\n",
    "                    person_element = wait_for_element(driver, By.XPATH, person_xpath, timeout=5)\n",
    "                    person_name = person_element.text.strip()\n",
    "                    metrics[\"People_Mentioned\"].append(person_name)\n",
    "                    print(f\"Person {index}: {person_name}\")\n",
    "                    index += 1\n",
    "                except Exception:\n",
    "                    print(f\"Finished retrieving people mentioned. Total: {len(metrics['People_Mentioned'])}\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving people mentioned: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def fetch_transcript(episode_url):\n",
    "    \"\"\"Fetch transcript from the given URL.\"\"\"\n",
    "    print(f\"Fetching transcript for {episode_url}...\")\n",
    "    try:\n",
    "        response = requests.get(episode_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        transcript_tag = soup.find(\"p\", class_=\"chakra-text ssc-transcript css-0\")\n",
    "        transcript = transcript_tag.get_text(strip=True) if transcript_tag else \"Transcript not available.\"\n",
    "\n",
    "        # Preprocess the transcript\n",
    "        return preprocess_transcript(transcript)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching transcript for {episode_url}: {e}\")\n",
    "        return \"Transcript not available.\"\n",
    "\n",
    "\n",
    "def preprocess_transcript(transcript):\n",
    "    \"\"\"Preprocess transcript by cleaning, lemmatizing, and tokenizing.\"\"\"\n",
    "    if transcript == \"Transcript not available.\":\n",
    "        return transcript\n",
    "\n",
    "    # Text Cleaning: Remove punctuation, stopwords, and lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    transcript = re.sub(r'[^a-zA-Z\\s]', '', transcript)  # Remove punctuation\n",
    "    words = word_tokenize(transcript.lower())\n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    cleaned_transcript = ' '.join(cleaned_words)\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(transcript)\n",
    "\n",
    "    return {\n",
    "        \"cleaned_transcript\": cleaned_transcript,\n",
    "        \"sentiment\": sentiment\n",
    "    }\n",
    "\n",
    "\n",
    "def process_youtube_data(input_file, output_file, json_output_file):\n",
    "    \"\"\"Process YouTube video URLs from an Excel file, save scraped data, and generate JSON.\"\"\"\n",
    "    df = pd.read_excel(input_file)\n",
    "    driver = setup_driver()\n",
    "    processed_data = []\n",
    "\n",
    "    try:\n",
    "        for index, row in df.iterrows():  \n",
    "            video_url = row['url']\n",
    "            generated_url = row['generated_url']\n",
    "            print(f\"Processing video: {row['name']}\")\n",
    "            try:\n",
    "                metrics = scrape_youtube_video_data(driver, video_url)\n",
    "                transcript_data = fetch_transcript(generated_url)\n",
    "\n",
    "                # Save the scraped data to the DataFrame\n",
    "                df.at[index, 'Likes'] = metrics[\"Likes\"]\n",
    "                df.at[index, 'Views'] = metrics[\"Views\"]\n",
    "                df.at[index, 'People_Mentioned'] = \", \".join(metrics[\"People_Mentioned\"])\n",
    "\n",
    "                # Append data for JSON file\n",
    "                processed_data.append({\n",
    "                    \"Episode Title\": row['name'],\n",
    "                    \"Cleaned_Transcript\": transcript_data[\"cleaned_transcript\"] if isinstance(transcript_data, dict) else transcript_data,\n",
    "                    \"people_mentioned\": metrics[\"People_Mentioned\"],\n",
    "                    \"views\": metrics[\"Views\"],\n",
    "                    \"likes\": metrics[\"Likes\"],\n",
    "                    \"guest\": row.get('guest', 'Unknown'),\n",
    "                    \"sentiment\": transcript_data.get(\"sentiment\") if isinstance(transcript_data, dict) else {}\n",
    "                })\n",
    "                print(f\"Successfully processed video: {row['name']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {row['name']}: {e}\")\n",
    "\n",
    "        # Save the updated DataFrame to an Excel file\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"Scraped data saved to {output_file}\")\n",
    "\n",
    "        # Save the processed data to a JSON file\n",
    "        with open(json_output_file, 'w') as json_file:\n",
    "            json.dump(processed_data, json_file, indent=4)\n",
    "        print(f\"Processed data saved to {json_output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"WebDriver closed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"jre_episodes.xlsx\"\n",
    "    output_file = \"jre_episodes_with_metrics.xlsx\"\n",
    "    json_output_file = \"jre_episodes_data.json\"\n",
    "    process_youtube_data(input_file, output_file, json_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique named entities: 9459\n",
      "Top 10 entities:\n",
      "[('Jamie', 814), ('Joe', 646), ('Dude', 471), ('Jesus', 415), ('COVID', 398), ('Twitter', 318), ('Trump', 277), ('Tony', 241), ('Biden', 222), ('Jesus Christ', 221)]\n",
      "Entity counts saved to entity_counts_sorted.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Load processed transcripts\n",
    "def load_transcripts(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Count named entities\n",
    "def count_named_entities(transcripts):\n",
    "    entity_counter = Counter()\n",
    "\n",
    "    for episode in transcripts:\n",
    "        entities = episode.get(\"Named_Entities\", [])\n",
    "        entity_counter.update(entities)  # Add the entities from each episode to the counter\n",
    "\n",
    "    return entity_counter\n",
    "\n",
    "# Save entity counts to a file in descending order\n",
    "def save_entity_counts(entity_counts, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entity, count in entity_counts.most_common():  # Automatically sorted in descending order\n",
    "            f.write(f\"{entity}: {count}\\n\")\n",
    "    print(f\"Entity counts saved to {output_file}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    input_file = \"processed_transcripts.json\"  # Update with your file path\n",
    "    output_file = \"entity_counts_sorted.txt\"\n",
    "\n",
    "    transcripts = load_transcripts(input_file)\n",
    "    entity_counts = count_named_entities(transcripts)\n",
    "\n",
    "    print(f\"Total unique named entities: {len(entity_counts)}\")\n",
    "    print(f\"Top 10 entities:\\n{entity_counts.most_common(10)}\")  # Print top 10 entities to console\n",
    "\n",
    "    save_entity_counts(entity_counts, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping saved to name_mapping.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "# Load entity counts from file\n",
    "def load_entity_counts(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    entities = {}\n",
    "    for line in lines:\n",
    "        match = re.match(r\"(.+): (\\d+)\", line.strip())  # Match \"Entity: Count\"\n",
    "        if match:\n",
    "            entity, count = match.groups()\n",
    "            entities[entity.strip()] = int(count)\n",
    "    return entities\n",
    "\n",
    "# Define custom corrections for specific cases\n",
    "custom_corrections = {\n",
    "    \"mike tyson\": \"Myke Tyson\",\n",
    "    \"this mike tyson\": \"Myke Tyson\",\n",
    "    \"the david carrot\": \"David Carrot\"\n",
    "}\n",
    "\n",
    "# Normalize entity name for grouping\n",
    "def normalize_entity(entity):\n",
    "    \"\"\"\n",
    "    Normalize entity names by:\n",
    "    - Removing prefixes like \"a\", \"an\", \"the\", \"this\" (e.g., \"the David Carrot\" -> \"David Carrot\")\n",
    "    - Removing possessive forms (e.g., \"Donald's\" -> \"Donald\")\n",
    "    - Removing trailing 's' where appropriate\n",
    "    - Ensuring names include at least a first and last name\n",
    "    - Applying custom corrections\n",
    "    \"\"\"\n",
    "    entity = entity.lower().strip()\n",
    "    entity = re.sub(r\"^(a|an|the|this)\\s+\", \"\", entity)  # Remove prefixes\n",
    "    entity = re.sub(r\"'s$\", \"\", entity)  # Remove possessive forms\n",
    "    entity = re.sub(r\"s$\", \"\", entity)  # Remove trailing 's' for plurals\n",
    "    entity = re.sub(r\"[^\\w\\s]\", \"\", entity)  # Remove non-alphanumeric characters\n",
    "\n",
    "    # Apply custom corrections if available\n",
    "    if entity in custom_corrections:\n",
    "        return custom_corrections[entity]\n",
    "\n",
    "    # Check if the entity includes at least a first and last name\n",
    "    if len(entity.split()) < 2:  # Remove names with fewer than 2 words\n",
    "        return None\n",
    "\n",
    "    # Capitalize each word in the name for consistency\n",
    "    return entity.title()\n",
    "\n",
    "# Group similar entities and create mapping\n",
    "def create_name_mapping(entities):\n",
    "    name_mapping = {}\n",
    "    grouped_entities = defaultdict(list)\n",
    "\n",
    "    for entity in entities.keys():\n",
    "        normalized = normalize_entity(entity)  # Normalize name\n",
    "        if normalized:  # Skip if None (e.g., single names)\n",
    "            grouped_entities[normalized].append(entity)\n",
    "\n",
    "    for normalized, variations in grouped_entities.items():\n",
    "        # Use the most common variation as the standard\n",
    "        standard_name = max(variations, key=lambda x: entities[x])\n",
    "        for variation in variations:\n",
    "            name_mapping[variation] = standard_name\n",
    "\n",
    "    return name_mapping\n",
    "\n",
    "# Save the mapping to a file\n",
    "def save_mapping_to_file(mapping, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for key, value in mapping.items():\n",
    "            file.write(f\"{key} -> {value}\\n\")\n",
    "    print(f\"Mapping saved to {output_file}\")\n",
    "\n",
    "# Main script\n",
    "entity_counts_file = \"entity_counts_sorted.txt\"\n",
    "output_mapping_file = \"name_mapping.txt\"\n",
    "\n",
    "entities = load_entity_counts(entity_counts_file)\n",
    "name_mapping = create_name_mapping(entities)\n",
    "save_mapping_to_file(name_mapping, output_mapping_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
