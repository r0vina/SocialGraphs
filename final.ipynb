{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "##I need toi get also the guest name\n",
    "\n",
    "base_url = \"https://ogjre.com/transcripts\"\n",
    "output_file = \"filtered_transcripts.txt\"\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Uncomment to run in headless mode\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def fetch_all_episodes(driver):\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)\n",
    "    print(\"Webpage loaded. Starting to scroll...\")\n",
    "\n",
    "    episodes = []\n",
    "    scroll_pause_time = 2\n",
    "    max_no_new_content_attempts = 10\n",
    "    no_new_content_attempts = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while no_new_content_attempts < max_no_new_content_attempts:\n",
    "        try:\n",
    "            episode_elements = driver.find_elements(By.CLASS_NAME, \"VideoSingle__VideoSingleStyles-sc-dngnuh-0\")\n",
    "            new_episodes_found = False\n",
    "\n",
    "            for episode in episode_elements:\n",
    "                try:\n",
    "                    link = episode.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                    title = episode.find_element(By.CLASS_NAME, \"vs-video-title\").text\n",
    "                    if (title, link) not in episodes:\n",
    "                        episodes.append((title, link))\n",
    "                        new_episodes_found = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting episode: {e}\")\n",
    "\n",
    "            if new_episodes_found:\n",
    "                no_new_content_attempts = 0\n",
    "            else:\n",
    "                no_new_content_attempts += 1\n",
    "\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scrolling: {e}\")\n",
    "            break\n",
    "\n",
    "    return episodes\n",
    "\n",
    "def fetch_transcript(episode_url):\n",
    "    print(f\"Fetching transcript for {episode_url}...\")\n",
    "    response = requests.get(episode_url)\n",
    "    if response.status_code != 200:\n",
    "        return \"Transcript not available.\"\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    transcript_tag = soup.find(\"p\", class_=\"chakra-text ssc-transcript css-0\")\n",
    "    return transcript_tag.get_text(strip=True) if transcript_tag else \"Transcript not available.\"\n",
    "\n",
    "def is_valid_episode_title(title):\n",
    "    return bool(re.match(r\"^#\\d+\\s*-\\s*.+\", title))\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        episodes = fetch_all_episodes(driver)\n",
    "        print(f\"Found {len(episodes)} episodes.\")\n",
    "\n",
    "        # Filter episodes by valid title format\n",
    "        valid_episodes = [(title, link) for title, link in episodes if is_valid_episode_title(title)]\n",
    "        print(f\"Found {len(valid_episodes)} valid episodes.\")\n",
    "\n",
    "        # Save filtered transcripts\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            for title, link in valid_episodes:\n",
    "                print(f\"Scraping episode: {title}\")\n",
    "                transcript = fetch_transcript(link)\n",
    "                file.write(f\"Episode Title: {title}\\n\")\n",
    "                file.write(f\"Transcript:\\n{transcript}\\n\")\n",
    "                file.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        print(f\"Filtered transcripts saved in {output_file}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove excessive whitespace\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Extract named entities (people)\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "# Perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment[\"compound\"]  # Return compound sentiment score\n",
    "\n",
    "# Preprocess each transcript\n",
    "def preprocess_transcripts(input_file, output_file):\n",
    "    processed_data = []\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        episodes = f.read().split(\"=\" * 80)  # Split episodes by separator\n",
    "\n",
    "    for episode in episodes:\n",
    "        if not episode.strip():\n",
    "            continue\n",
    "        lines = episode.strip().split(\"\\n\")\n",
    "        title = lines[0].replace(\"Episode Title: \", \"\").strip()\n",
    "        transcript = \"\\n\".join(lines[1:]).replace(\"Transcript:\\n\", \"\").strip()\n",
    "\n",
    "        cleaned_transcript = clean_text(transcript)\n",
    "        named_entities = extract_named_entities(transcript)\n",
    "        sentiment_score = analyze_sentiment(transcript)\n",
    "\n",
    "        processed_data.append({\n",
    "            \"Episode Title\": title,\n",
    "            \"Cleaned_Transcript\": cleaned_transcript,\n",
    "            \"Named_Entities\": named_entities,\n",
    "            \"Sentiment_Score\": sentiment_score\n",
    "        })\n",
    "\n",
    "    # Save processed data to a JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed_data, f, indent=4)\n",
    "\n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "preprocess_transcripts(\"joe_rogan_filtered_transcript.txt\", \"processed_transcripts.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique named entities: 9459\n",
      "Top 10 entities:\n",
      "[('Jamie', 814), ('Joe', 646), ('Dude', 471), ('Jesus', 415), ('COVID', 398), ('Twitter', 318), ('Trump', 277), ('Tony', 241), ('Biden', 222), ('Jesus Christ', 221)]\n",
      "Entity counts saved to entity_counts_sorted.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Load processed transcripts\n",
    "def load_transcripts(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Count named entities\n",
    "def count_named_entities(transcripts):\n",
    "    entity_counter = Counter()\n",
    "\n",
    "    for episode in transcripts:\n",
    "        entities = episode.get(\"Named_Entities\", [])\n",
    "        entity_counter.update(entities)  # Add the entities from each episode to the counter\n",
    "\n",
    "    return entity_counter\n",
    "\n",
    "# Save entity counts to a file in descending order\n",
    "def save_entity_counts(entity_counts, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entity, count in entity_counts.most_common():  # Automatically sorted in descending order\n",
    "            f.write(f\"{entity}: {count}\\n\")\n",
    "    print(f\"Entity counts saved to {output_file}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    input_file = \"processed_transcripts.json\"  # Update with your file path\n",
    "    output_file = \"entity_counts_sorted.txt\"\n",
    "\n",
    "    transcripts = load_transcripts(input_file)\n",
    "    entity_counts = count_named_entities(transcripts)\n",
    "\n",
    "    print(f\"Total unique named entities: {len(entity_counts)}\")\n",
    "    print(f\"Top 10 entities:\\n{entity_counts.most_common(10)}\")  # Print top 10 entities to console\n",
    "\n",
    "    save_entity_counts(entity_counts, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping saved to name_mapping.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "# Load entity counts from file\n",
    "def load_entity_counts(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    entities = {}\n",
    "    for line in lines:\n",
    "        match = re.match(r\"(.+): (\\d+)\", line.strip())  # Match \"Entity: Count\"\n",
    "        if match:\n",
    "            entity, count = match.groups()\n",
    "            entities[entity.strip()] = int(count)\n",
    "    return entities\n",
    "\n",
    "# Define custom corrections for specific cases\n",
    "custom_corrections = {\n",
    "    \"mike tyson\": \"Myke Tyson\",\n",
    "    \"this mike tyson\": \"Myke Tyson\",\n",
    "    \"the david carrot\": \"David Carrot\"\n",
    "}\n",
    "\n",
    "# Normalize entity name for grouping\n",
    "def normalize_entity(entity):\n",
    "    \"\"\"\n",
    "    Normalize entity names by:\n",
    "    - Removing prefixes like \"a\", \"an\", \"the\", \"this\" (e.g., \"the David Carrot\" -> \"David Carrot\")\n",
    "    - Removing possessive forms (e.g., \"Donald's\" -> \"Donald\")\n",
    "    - Removing trailing 's' where appropriate\n",
    "    - Ensuring names include at least a first and last name\n",
    "    - Applying custom corrections\n",
    "    \"\"\"\n",
    "    entity = entity.lower().strip()\n",
    "    entity = re.sub(r\"^(a|an|the|this)\\s+\", \"\", entity)  # Remove prefixes\n",
    "    entity = re.sub(r\"'s$\", \"\", entity)  # Remove possessive forms\n",
    "    entity = re.sub(r\"s$\", \"\", entity)  # Remove trailing 's' for plurals\n",
    "    entity = re.sub(r\"[^\\w\\s]\", \"\", entity)  # Remove non-alphanumeric characters\n",
    "\n",
    "    # Apply custom corrections if available\n",
    "    if entity in custom_corrections:\n",
    "        return custom_corrections[entity]\n",
    "\n",
    "    # Check if the entity includes at least a first and last name\n",
    "    if len(entity.split()) < 2:  # Remove names with fewer than 2 words\n",
    "        return None\n",
    "\n",
    "    # Capitalize each word in the name for consistency\n",
    "    return entity.title()\n",
    "\n",
    "# Group similar entities and create mapping\n",
    "def create_name_mapping(entities):\n",
    "    name_mapping = {}\n",
    "    grouped_entities = defaultdict(list)\n",
    "\n",
    "    for entity in entities.keys():\n",
    "        normalized = normalize_entity(entity)  # Normalize name\n",
    "        if normalized:  # Skip if None (e.g., single names)\n",
    "            grouped_entities[normalized].append(entity)\n",
    "\n",
    "    for normalized, variations in grouped_entities.items():\n",
    "        # Use the most common variation as the standard\n",
    "        standard_name = max(variations, key=lambda x: entities[x])\n",
    "        for variation in variations:\n",
    "            name_mapping[variation] = standard_name\n",
    "\n",
    "    return name_mapping\n",
    "\n",
    "# Save the mapping to a file\n",
    "def save_mapping_to_file(mapping, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for key, value in mapping.items():\n",
    "            file.write(f\"{key} -> {value}\\n\")\n",
    "    print(f\"Mapping saved to {output_file}\")\n",
    "\n",
    "# Main script\n",
    "entity_counts_file = \"entity_counts_sorted.txt\"\n",
    "output_mapping_file = \"name_mapping.txt\"\n",
    "\n",
    "entities = load_entity_counts(entity_counts_file)\n",
    "name_mapping = create_name_mapping(entities)\n",
    "save_mapping_to_file(name_mapping, output_mapping_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
