{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Part - Scrapping the Data, do NOT run this\n",
    "\n",
    "This script is designed to **scrape data from YouTube videos** and **analyze transcripts** to gain more insights.\n",
    "\n",
    "### **Libraries**\n",
    "- The script begins by importing the necessary libraries. These include:\n",
    "  - **Selenium**: Used for automating web interactions (like clicking buttons and scrolling).\n",
    "  - **BeautifulSoup**: Used for parsing HTML content and extracting specific parts of a webpage.\n",
    "  - **NLTK**: Used for text processing, including cleaning and analyzing the text.\n",
    "- A **Chrome WebDriver** is set up using Selenium to control the browser. This driver can open a YouTube video and interact with it, like clicking buttons or scrolling.\n",
    "\n",
    "### **Web Scraping YouTube Video Data**\n",
    "- The script visits a YouTube video page and extracts information such as:\n",
    "  - **Likes**: The number of likes on the video.\n",
    "  - **Views**: The total number of views.\n",
    "  - **People Mentioned**: Names of individuals mentioned in the description or title of the video.\n",
    "- For example, if the video has **10,000 likes** and **500,000 views**, this function will gather those numbers. If the description mentions people like \"Joe Rogan\" and \"Elon Musk,\" those names are also captured.\n",
    "\n",
    "### **Fetching and Processing the Transcript**\n",
    "- **`fetch_transcript(episode_url)`**: This function fetches the transcript of a video from athis website https://ogjre.com/transcripts\n",
    "\n",
    "### **Text Processing**\n",
    "- **`preprocess_transcript(transcript)`**: This function prepares the transcript for further analysis:\n",
    "  - **Text Cleaning**: Removes punctuation marks and converts the text to lowercase.\n",
    "  - **Tokenization and Lemmatization**: Breaks the transcript into individual words and reduces them to their root form. For example, *\"running\"* becomes *\"run\"*.\n",
    "  - **Stopword Removal**: Common words like \"the\", \"and\", \"is\" are removed since they donâ€™t add much meaning.\n",
    "- Once the transcript is cleaned, **Sentiment Analysis** is performed using **VADER** from NLTK. This analysis provides four scores:\n",
    "  - **Negative (neg)**: Measures the negative tone in the text.\n",
    "  - **Neutral (neu)**: Measures neutral content.\n",
    "  - **Positive (pos)**: Measures the positive tone.\n",
    "  - **Compound**: A combined score that shows the overall sentiment (ranging from -1 to 1, where 1 means very positive).\n",
    "- For example, if the transcript talks about positive topics like *\"exciting future\"*, the sentiment analysis might return a positive score.\n",
    "\n",
    "\n",
    "### **Main Processing Function**\n",
    "- **`process_youtube_data(input_file, output_file, json_output_file)`**: This is the main function that:\n",
    "  - Loads YouTube video URLs from an Excel file.\n",
    "  - Uses **`scrape_youtube_video_data`** to gather information about each video.\n",
    "  - Uses **`fetch_transcript`** to retrieve and analyze the transcript of each video.\n",
    "  - Updates the Excel file and JSON file with the scraped data.\n",
    "- For example, if the Excel file contains a list of 20 YouTube URLs, this function will visit each link, collect information, analyze it, and save the results.\n",
    "\n",
    "### **Output**\n",
    "- The script produces two main outputs:\n",
    "  1. **Excel File** (`jre_episodes_data.xlsx`): Contains updated information about each video, such as likes, views, and people mentioned.\n",
    "  2. **JSON File** (`updated_jre_episodes_data.json`): Contains more detailed data, including the cleaned transcript and sentiment analysis.\n",
    "\n",
    "\n",
    "### Example of Output Data\n",
    "- **Transcript and Sentiment**: The output includes a cleaned version of the transcript, along with sentiment scores like:\n",
    "  ```json\n",
    "  {\n",
    "      \"Episode Title\": \"Joe Rogan Experience #2159 - Sal Vulcano\",\n",
    "      \"Cleaned_Transcript\": \"hello everyone today we talk about exciting future space\",\n",
    "      \"people_mentioned\": [\"Sal Vulcano\"],\n",
    "      \"views\": 500000,\n",
    "      \"likes\": 10000,\n",
    "      \"guest\": [\"Sal Vulcano\"],\n",
    "      \"sentiment\": {\n",
    "          \"neg\": 0.093,\n",
    "          \"neu\": 0.719,\n",
    "          \"pos\": 0.188,\n",
    "          \"compound\": 1.0\n",
    "      }\n",
    "  }\n",
    "\n",
    "### Sentiment Explanation\n",
    "\n",
    "Negative (neg): 0.093 indicates 9.3% of the content is negative.\n",
    "Neutral (neu): 0.719 indicates 71.9% of the content is neutral.\n",
    "Positive (pos): 0.188 indicates 18.8% of the content is positive.\n",
    "Compound: 1.0 means the overall sentiment is very positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just look at the json file were the transcripts are included and there is also an excel file to have a cleare visualization of the output\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "# Setup WebDriver with the same configuration\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Uncomment the following line to run in headless mode\n",
    "    # options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--start-maximized\")  # Open browser in fullscreen\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "def wait_for_element(driver, by, value, timeout=15):\n",
    "    \"\"\"Wait for an element to be present on the page.\"\"\"\n",
    "    return WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, value)))\n",
    "\n",
    "\n",
    "def scrape_youtube_video_data(driver, url):\n",
    "    \"\"\"Scrape YouTube video data including likes, views, and people mentioned.\"\"\"\n",
    "    metrics = {\"Likes\": \"Not Found\", \"Views\": \"Not Found\", \"People_Mentioned\": []}\n",
    "    try:\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "        # Accept cookies if prompted\n",
    "        try:\n",
    "            cookie_button = wait_for_element(driver, By.XPATH, '//*[@id=\"content\"]/div[2]/div[6]/div[1]/ytd-button-renderer[1]/yt-button-shape/button/yt-touch-feedback-shape/div/div[2]', timeout=10)\n",
    "            cookie_button.click()\n",
    "            print(\"Cookies accepted.\")\n",
    "            time.sleep(2)\n",
    "        except Exception:\n",
    "            print(\"No cookies prompt or already accepted.\")\n",
    "\n",
    "        # Get likes\n",
    "        try:\n",
    "            likes_element = wait_for_element(driver, By.XPATH, '//*[@id=\"top-level-buttons-computed\"]/segmented-like-dislike-button-view-model/yt-smartimation/div/div/like-button-view-model/toggle-button-view-model/button-view-model/button/div[2]', 15)\n",
    "            metrics[\"Likes\"] = likes_element.text\n",
    "            print(f\"Likes: {metrics['Likes']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch likes: {e}\")\n",
    "\n",
    "        # Get views\n",
    "        try:\n",
    "            views_element = wait_for_element(driver, By.CSS_SELECTOR, '.style-scope.yt-formatted-string.bold[style-target=\"bold\"]', 15)\n",
    "            metrics[\"Views\"] = views_element.text\n",
    "            print(f\"Views: {metrics['Views']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch views: {e}\")\n",
    "\n",
    "        # Expand and retrieve names of people mentioned\n",
    "        try:\n",
    "            expand_button = wait_for_element(driver, By.XPATH, '//*[@id=\"expand\"]', timeout=10)\n",
    "            expand_button.click()\n",
    "            print(\"'Expand' button clicked. Scrolling down to reveal names.\")\n",
    "            time.sleep(2)  # Allow time for content to expand\n",
    "\n",
    "            # Scroll down to ensure all names are loaded\n",
    "            driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Retrieve names of people mentioned\n",
    "            index = 1\n",
    "            while True:\n",
    "                try:\n",
    "                    person_xpath = f'//*[@id=\"items\"]/yt-video-attributes-section-view-model/div/div[2]/div/yt-video-attribute-view-model[{index}]/div/a/div[2]/h1'\n",
    "                    person_element = wait_for_element(driver, By.XPATH, person_xpath, timeout=5)\n",
    "                    person_name = person_element.text.strip()\n",
    "                    metrics[\"People_Mentioned\"].append(person_name)\n",
    "                    print(f\"Person {index}: {person_name}\")\n",
    "                    index += 1\n",
    "                except Exception:\n",
    "                    print(f\"Finished retrieving people mentioned. Total: {len(metrics['People_Mentioned'])}\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving people mentioned: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def fetch_transcript(episode_url):\n",
    "    \"\"\"Fetch transcript from the given URL.\"\"\"\n",
    "    print(f\"Fetching transcript for {episode_url}...\")\n",
    "    try:\n",
    "        response = requests.get(episode_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        transcript_tag = soup.find(\"p\", class_=\"chakra-text ssc-transcript css-0\")\n",
    "        transcript = transcript_tag.get_text(strip=True) if transcript_tag else \"Transcript not available.\"\n",
    "\n",
    "        # Preprocess the transcript\n",
    "        return preprocess_transcript(transcript)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching transcript for {episode_url}: {e}\")\n",
    "        return \"Transcript not available.\"\n",
    "\n",
    "\n",
    "def preprocess_transcript(transcript):\n",
    "    \"\"\"Preprocess transcript by cleaning, lemmatizing, and tokenizing.\"\"\"\n",
    "    if transcript == \"Transcript not available.\":\n",
    "        return transcript\n",
    "\n",
    "    # Text Cleaning: Remove punctuation, stopwords, and lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    transcript = re.sub(r'[^a-zA-Z\\s]', '', transcript)  # Remove punctuation\n",
    "    words = word_tokenize(transcript.lower())\n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    cleaned_transcript = ' '.join(cleaned_words)\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(transcript)\n",
    "\n",
    "    return {\n",
    "        \"cleaned_transcript\": cleaned_transcript,\n",
    "        \"sentiment\": sentiment\n",
    "    }\n",
    "\n",
    "\n",
    "def save_intermediate_results(df, processed_data, output_file, json_output_file):\n",
    "    \"\"\"Save the current state of the DataFrame and JSON data to avoid data loss.\"\"\"\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Intermediate data saved to {output_file}\")\n",
    "    with open(json_output_file, 'w') as json_file:\n",
    "        json.dump(processed_data, json_file, indent=4)\n",
    "    print(f\"Intermediate JSON data saved to {json_output_file}\")\n",
    "\n",
    "\n",
    "def process_youtube_data(input_file, output_file, json_output_file):\n",
    "    \"\"\"Process YouTube video URLs from an Excel file, save scraped data, and generate JSON.\"\"\"\n",
    "    # Load the existing data if available\n",
    "    if os.path.exists(output_file):\n",
    "        df = pd.read_excel(output_file)\n",
    "        print(f\"Loaded existing data from {output_file}\")\n",
    "    else:\n",
    "        df = pd.read_excel(input_file)\n",
    "\n",
    "    if os.path.exists(json_output_file):\n",
    "        with open(json_output_file, 'r') as json_file:\n",
    "            processed_data = json.load(json_file)\n",
    "        print(f\"Loaded existing JSON data from {json_output_file}\")\n",
    "    else:\n",
    "        processed_data = []\n",
    "\n",
    "    driver = setup_driver()\n",
    "\n",
    "    try:\n",
    "        for index, row in df.iterrows():\n",
    "            if pd.notna(row.get('Likes')) and pd.notna(row.get('Views')) and pd.notna(row.get('People_Mentioned')):\n",
    "                # Skip rows that have already been processed\n",
    "                continue\n",
    "\n",
    "            video_url = row['url']\n",
    "            generated_url = row['generated_url']\n",
    "            print(f\"Processing video: {row['name']}\")\n",
    "            try:\n",
    "                metrics = scrape_youtube_video_data(driver, video_url)\n",
    "                transcript_data = fetch_transcript(generated_url)\n",
    "\n",
    "                # Save the scraped data to the DataFrame\n",
    "                df.at[index, 'Likes'] = metrics[\"Likes\"]\n",
    "                df.at[index, 'Views'] = metrics[\"Views\"]\n",
    "                df.at[index, 'People_Mentioned'] = \", \".join(metrics[\"People_Mentioned\"])\n",
    "\n",
    "                # Append data for JSON file\n",
    "                processed_data.append({\n",
    "                    \"Episode Title\": row['name'],\n",
    "                    \"Cleaned_Transcript\": transcript_data[\"cleaned_transcript\"] if isinstance(transcript_data, dict) else transcript_data,\n",
    "                    \"people_mentioned\": metrics[\"People_Mentioned\"],\n",
    "                    \"views\": metrics[\"Views\"],\n",
    "                    \"likes\": metrics[\"Likes\"],\n",
    "                    \"guest\": row.get('guest', 'Unknown'),\n",
    "                    \"sentiment\": transcript_data.get(\"sentiment\") if isinstance(transcript_data, dict) else {}\n",
    "                })\n",
    "                print(f\"Successfully processed video: {row['name']}\")\n",
    "\n",
    "                # Save intermediate results after processing each video\n",
    "                save_intermediate_results(df, processed_data, output_file, json_output_file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {row['name']}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"WebDriver closed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"jre_episodes.xlsx\"\n",
    "    output_file = \"jre_episodes_with_metrics.xlsx\"\n",
    "    json_output_file = \"jre_episodes_data.json\"\n",
    "    process_youtube_data(input_file, output_file, json_output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps \n",
    "\n",
    "\n",
    "## How to Use the Outputs of the Script\n",
    "\n",
    "The script produces two main outputs: an Excel file and a JSON file. Here is how you can use them for the next steps:\n",
    "\n",
    "### **Excel File (`jre_episodes_data.xlsx`)**:\n",
    "   - **Guest Standardization**: Use the guest column to standardize guest names. Correct any inconsistencies to make sure that the same guest is represented uniformly.\n",
    "   - **Engagement Metrics**: Analyze likes and views to understand which episodes were more popular. This can help in the engagement vs. sentiment analysis.\n",
    "\n",
    "### **JSON File (`updated_jre_episodes_data.json`)**:\n",
    "   - **Sentiment Analysis and Transcript**:\n",
    "     - Extract and use the cleaned transcript and sentiment scores for each episode. This data is essential for topic modeling and understanding audience sentiment.\n",
    "   - **Network Construction**:\n",
    "     - Use the guest names and episode information from the JSON file to create a network of interconnected episodes.\n",
    "   - **Correlation Analysis**:\n",
    "     - Use the engagement metrics (likes, views) alongside the sentiment scores to perform correlation analysis.\n",
    "\n",
    "### **Further Analysis and Insights**:\n",
    "   - Use the cleaned transcripts to perform **topic modeling** and **thematic analysis**.\n",
    "   - Analyze sentiment trends across different themes and correlate them with the guest appearances and audience engagement data.\n",
    "   - Construct **network graphs** based on shared guests and topics to visualize relationships between different episodes.\n",
    "   - Generate **visual insights** such as word clouds, scatter plots, and sentiment trends to help in interpreting the data.\n",
    "\n",
    "### **Visualizations**:\n",
    "   - Visualize the relationships between **engagement metrics, guests, and sentiment** using charts.\n",
    "   - Create a **network graph** to see which guests often appear together or how episodes are connected by shared topics.\n",
    "   - **Topic Clouds** can be generated to see which words are most often associated with specific guests or themes.\n",
    "\n",
    "By following these steps and using the outputs effectively, you will be able to conduct a thorough analysis of the Joe Rogan Experience podcast, extracting insights into its content, audience engagement, and thematic relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
